{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-publicity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", numGPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D,Dropout,Activation,MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import talos\n",
    "import os\n",
    "import math\n",
    "import threading\n",
    "from matplotlib import pyplot\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "numClients = 50\n",
    "num_classes = 10\n",
    "dataset_name = 'svhn_cropped'\n",
    "experiment_name = 'svhn'\n",
    "Path(experiment_name+\"_res/intervals_res\"+str(numClients)).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters for grid search over activation function\n",
    "degree = 3\n",
    "interval = 0\n",
    "\n",
    "def reluApprox(x):\n",
    "    if degree == 3:  \n",
    "        if interval == 3:\n",
    "            return 0.7146 + 1.5000*K.pow(x/interval,1)+0.8793*K.pow(x/interval,2)\n",
    "        if interval == 5:\n",
    "            return 0.7865 + 2.5000*K.pow(x/interval,1)+1.88*K.pow(x/interval,2)\n",
    "        if interval == 7:\n",
    "            return 0.9003 + 3.5000*K.pow(x/interval,1)+2.9013*K.pow(x/interval,2)\n",
    "        if interval == 10:\n",
    "            return 1.1155 + 5*K.pow(x/interval,1)+4.4003*K.pow(x/interval,2)\n",
    "        if interval == 12:\n",
    "            return 1.2751 + 6*K.pow(x/interval,1)+5.3803*K.pow(x/interval,2)\n",
    "    if degree == 5:  \n",
    "        if interval == 7:\n",
    "            return 0.7521 + 3.5000*K.pow(x/interval,1)+4.3825*K.pow(x/interval,2)-1.7281*K.pow(x/interval,4)\n",
    "        if interval == 20:\n",
    "            return 1.3127 + 10*K.pow(x/interval,1)+15.7631*K.pow(x/interval,2)-7.6296*K.pow(x/interval,4)\n",
    "def sigmoidApprox(x):\n",
    "    if degree == 3:  \n",
    "        if interval == 3:\n",
    "            return 0.5 + 0.6997*K.pow(x/interval,1)-0.2649*K.pow(x/interval,3)\n",
    "        if interval == 5:\n",
    "            return 0.5 + 0.9917*K.pow(x/interval,1)-0.5592*K.pow(x/interval,3)\n",
    "        if interval == 7:\n",
    "            return 0.5 + 1.1511*K.pow(x/interval,1)-0.7517*K.pow(x/interval,3)\n",
    "        if interval == 8:\n",
    "            return 0.5 + 1.2010*K.pow(x/interval,1)-0.8156*K.pow(x/interval,2)\n",
    "        if interval == 12:\n",
    "            return 0.5 + 1.2384*K.pow(x/interval,1)-0.8647*K.pow(x/interval,2)\n",
    "def tanApprox(x):\n",
    "    if degree == 3: \n",
    "        if interval == 1:\n",
    "            return 0.9797*K.pow(x/interval,1)-0.2268*K.pow(x/interval,3)        \n",
    "        if interval == 2:\n",
    "            return 1.7329*K.pow(x/interval,1)-0.8454*K.pow(x/interval,3)\n",
    "        if interval == 3:\n",
    "            return 2.1673*K.pow(x/interval,1)-1.3358*K.pow(x/interval,3)\n",
    "        if interval == 5:\n",
    "            return 2.5338*K.pow(x/interval,1)-1.8051*K.pow(x/interval,3)\n",
    "        if interval == 7:\n",
    "            return 2.6629*K.pow(x/interval,1) -1.9801*K.pow(x/interval,3)\n",
    "        if interval == 12:\n",
    "            return 2.7599*K.pow(x/interval,1)-2.1140*K.pow(x/interval,2)\n",
    "    if degree == 12:\n",
    "        print('ooopssss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set intervals to search, check intervals in the approx function\n",
    "intervals = [1,2,3,5,7,12]\n",
    "\n",
    "# Set approximated activation function and best parameters\n",
    "approx_act_fn = [tanApprox]\n",
    "intervals_params = dict(learn_rate=[0.193],\n",
    "                        batch_size=[8],\n",
    "                        momentum=[0.776],\n",
    "                        epochs=[29], \n",
    "                        act_fn=approx_act_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5,5), activation=params['act_fn'], input_shape=input_shape,name=\"1\"))\n",
    "    model.add(AveragePooling2D((2,2),name=\"2\"))\n",
    "    model.add(Conv2D(16, (5,5), activation=params['act_fn'],name=\"3\"))\n",
    "    model.add(AveragePooling2D((2,2),name=\"4\"))\n",
    "    model.add(Flatten(name=\"5\"))\n",
    "    model.add(Dense(120, activation=params['act_fn'],name=\"6\"))\n",
    "    model.add(Dense(84, activation=params['act_fn'],name=\"7\"))\n",
    "    model.add(Dense(num_classes, activation='softmax',name=\"f\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-danger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tfds.as_numpy(tfds.load(dataset_name,\n",
    "                                                               split=['train','test'],\n",
    "                                                               batch_size=-1,\n",
    "                                                               as_supervised=True))\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "#x_train = np.expand_dims(x_train, -1)\n",
    "#x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "y_test_uncat = y_test\n",
    "\n",
    "#to use with mean sq error\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "#shuffle both train and test once (to average between runs later on..)\n",
    "shuffler = np.random.permutation(len(x_train))\n",
    "x_train = x_train[shuffler]\n",
    "y_train = y_train[shuffler]\n",
    "shuffler = np.random.permutation(len(x_test))\n",
    "x_test = x_test[shuffler]\n",
    "y_test = y_test[shuffler]\n",
    "\n",
    "sample_height = x_train[0].shape[0]\n",
    "sample_width = x_train[0].shape[1]\n",
    "sample_channels = x_train[0].shape[2]\n",
    "\n",
    "\n",
    "input_shape = (sample_height, sample_width, sample_channels)\n",
    "\n",
    "def randomSplitClientsData(data,labels,numParties):\n",
    "    numSamplesPerClient = int(data.shape[0]/numParties)\n",
    "    print(numSamplesPerClient)\n",
    "    clientsData = np.zeros((numParties,int(numSamplesPerClient),sample_height,sample_width,sample_channels))\n",
    "    clientsDataLabels = np.zeros((numParties,int(numSamplesPerClient),num_classes))\n",
    "    #print(numSamplesPerClient)\n",
    "    ind = 0\n",
    "    for i in range(numParties):\n",
    "        clientsData[i] = data[ind:ind+numSamplesPerClient]\n",
    "        clientsDataLabels[i]=labels[ind:ind+numSamplesPerClient]\n",
    "        ind = ind+numSamplesPerClient\n",
    "    return clientsData, clientsDataLabels\n",
    "\n",
    "def prepare_data_for_X_clients(numClients):\n",
    "    clientsData, clientsDataLabels = randomSplitClientsData(x_train, y_train, numClients)\n",
    "    return clientsData, clientsDataLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history, params):\n",
    "    print('##########################################################')\n",
    "    print(params)\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('MSE')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    #filename = sys.argv[0].split('/')[-1]\n",
    "    #pyplot.savefig(filename + '_plot.png')\n",
    "    #pyplot.close()\n",
    "    pyplot.show()\n",
    "    print('##########################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "hist_params = []\n",
    "\n",
    "def experiment(x_train, y_train, x_val, y_val, params):\n",
    "        \n",
    "    \n",
    "    optimizer = SGD(learning_rate=params['learn_rate'], momentum=params['momentum'], nesterov=False, name='SGD')\n",
    "    \n",
    "    model = get_model(params)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall(),tf.keras.metrics.Precision()],\n",
    "                  run_eagerly=False)\n",
    "\n",
    "    history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=0)\n",
    "\n",
    "    hist.append(history)\n",
    "    hist_params.append(params)\n",
    "        \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_for_X_clients(numClients, clientsData, clientsDataLabels, param_grid):\n",
    "\n",
    "    scan_res = np.zeros(numClients, dtype=object)\n",
    "    \n",
    "    def client_gridsearch(i):\n",
    "        #Distribute load accross GPUs\n",
    "        with tf.device('/GPU:'+str(i%numGPUs)):\n",
    "\n",
    "            print('training in client ', i, 'interval is',interval)\n",
    "            scan_results = talos.Scan(x=clientsData[i],\n",
    "                                      y=clientsDataLabels[i],\n",
    "                                      params=param_grid,\n",
    "                                      model=experiment,\n",
    "                                      experiment_name=experiment_name)\n",
    "            scan_res[i]=scan_results\n",
    "            \n",
    "        \n",
    "    # Batch multithreading\n",
    "    n_batch = int(math.ceil(float(numClients)/(numGPUs)))\n",
    "    print(n_batch)\n",
    "    remaining_clients = numClients\n",
    "    for i in range(n_batch):\n",
    "        threads = list()\n",
    "\n",
    "        for j in range(min(numGPUs, remaining_clients)):\n",
    "            t = threading.Thread(target=client_gridsearch, args=(i*(numGPUs)+j,))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "        for _,t in enumerate(threads):\n",
    "            remaining_clients -= 1\n",
    "            t.join()\n",
    "    \n",
    "    return scan_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-johnson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interval_res = list()\n",
    "clientsData, clientsDataLabels = prepare_data_for_X_clients(numClients)\n",
    "\n",
    "for i in intervals:\n",
    "    global interval\n",
    "    interval = i\n",
    "    \n",
    "    interval_res.append(grid_search_for_X_clients(numClients, clientsData, clientsDataLabels, intervals_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_interval = 0\n",
    "\n",
    "choice_ratio = np.zeros(numClients, dtype=object)\n",
    "choice_ratio.fill((0,0.0))\n",
    "\n",
    "for index, i_res in enumerate(interval_res):\n",
    "    \n",
    "    i_res = [r for r in i_res if not r == 0]\n",
    "    \n",
    "    for c, client_scan in enumerate(i_res):\n",
    "        if not client_scan == 0:\n",
    "            curr_acc = client_scan.data.head(1)['val_accuracy'].item()\n",
    "\n",
    "            display(client_scan.data)\n",
    "\n",
    "            if choice_ratio[c][1] <= curr_acc:\n",
    "                choice_ratio[c] = (intervals[index], curr_acc)      \n",
    "\n",
    "            ## Write dataframes to files\n",
    "            client_scan.data.to_csv(experiment_name+\"_res/intervals_res\"+str(numClients)+\"/intervals_res_client\"+str(c)+\"_interval_\"+str(intervals[index])+\".csv\")\n",
    "\n",
    "choice_ratio = np.array([e[0] for e in choice_ratio])\n",
    "\n",
    "res_ratio = np.zeros(len(intervals), dtype=float)\n",
    "\n",
    "for i,e in enumerate(intervals):\n",
    "    res_ratio[i] = np.count_nonzero(choice_ratio == e)\n",
    "\n",
    "res_ratio = res_ratio / numClients\n",
    "\n",
    "best_interval = intervals[np.argmax(res_ratio)]\n",
    "print(res_ratio, best_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-knight",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_metrics_for_X_clients(numClients, clientsData, clientsDataLabels, avg_test_params):\n",
    "\n",
    "    metrics_res = np.zeros(numClients, dtype=object)\n",
    "    \n",
    "    \n",
    "    def client_test_metrics(i):\n",
    "        #Distribute load accross GPUs\n",
    "        with tf.device('/GPU:'+str(i%numGPUs)):\n",
    "\n",
    "            print('training in client ', i, 'interval is',interval)\n",
    "            model = get_model(avg_test_params)\n",
    "            optimizer = SGD(learning_rate=avg_test_params['learn_rate'], momentum=avg_test_params['momentum'], nesterov=False, name='SGD')\n",
    "\n",
    "            model.compile(optimizer=optimizer,\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall(),tf.keras.metrics.Precision()],\n",
    "                  run_eagerly=False)\n",
    "            model.fit(x=clientsData[i],\n",
    "                        y=clientsDataLabels[i],\n",
    "                        epochs=avg_test_params['epochs'],\n",
    "                        batch_size=avg_test_params['batch_size'],\n",
    "                        verbose=0)\n",
    "            print('evaluation in client ', i)\n",
    "            metrics = model.evaluate(x_test, y_test)\n",
    "            metrics_res[i] = metrics\n",
    "            \n",
    "        \n",
    "    # Batch multithreading\n",
    "    n_batch = int(math.ceil(float(numClients)/(numGPUs)))\n",
    "    print(n_batch)\n",
    "    remaining_clients = numClients\n",
    "    for i in range(n_batch):\n",
    "        threads = list()\n",
    "\n",
    "        for j in range(min(numGPUs, remaining_clients)):\n",
    "            t = threading.Thread(target=client_test_metrics, args=(i*(numGPUs)+j,))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "        for _,t in enumerate(threads):\n",
    "            remaining_clients -= 1\n",
    "            t.join()\n",
    "    \n",
    "    return metrics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-manual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### Retrain each client to run the test set and get best metrics\n",
    "intervals_test_res = []\n",
    "interval = best_interval\n",
    "intervals_params_test = dict(learn_rate=intervals_params['learn_rate'][0],\n",
    "                        batch_size=intervals_params['batch_size'][0],\n",
    "                        momentum=intervals_params['momentum'][0],\n",
    "                        epochs=intervals_params['epochs'][0], \n",
    "                        act_fn=intervals_params['act_fn'][0])\n",
    "\n",
    "clientsData, clientsDataLabels = prepare_data_for_X_clients(numClients)\n",
    "\n",
    "intervals_test_res = test_metrics_for_X_clients(numClients, clientsData, clientsDataLabels, intervals_params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = 1\n",
    "intervals_test_res = [r for r in intervals_test_res if not r == 0]\n",
    "for index, i_res in enumerate(intervals_test_res):\n",
    "    \n",
    "    if i_res[1] >= intervals_test_res[best_index][1]:\n",
    "        best_index = index\n",
    "            \n",
    "print(\"Best test accuracy :\", intervals_test_res[best_index][1])\n",
    "print(\"Loss :\", intervals_test_res[best_index][0])\n",
    "print(\"Precision :\", intervals_test_res[best_index][2])\n",
    "print(\"Recall :\", intervals_test_res[best_index][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intervals_params_test)\n",
    "print(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-valley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
