{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-publicity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", numGPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Conv2D,Dropout,Activation,MaxPooling2D,AveragePooling2D,GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import talos\n",
    "import os\n",
    "import math\n",
    "import threading\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import collections\n",
    "from pathlib import Path\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "numClients = 20\n",
    "num_classes = 10\n",
    "dataset_name = 'svhn_cropped'\n",
    "experiment_name = 'svhn'\n",
    "Path(experiment_name+\"_res/res\"+str(numClients)).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "def get_params_for_X_clients(x):    \n",
    "    act_fn = [\"relu\"]\n",
    "    \n",
    "    batch_size = [64]#[32,16,8,4,2]#[32,64,128]#[8,16,32]#[256,128,64,32]\n",
    "    epochs = [125]#[75,125]\n",
    "    learn_rate = [0.1]#[0.1,0.15,0.2,0.25,0.3]#[0.08, 0.1, 0.2, 0.3]#[0.001,0.01, 0.1]\n",
    "    momentum = [0.9]#[0.7,0.8,0.85,0.9,0.95]#[0.88, 0.89, 0.90, 0.91, 0.92, 0.93, 0.94]#[0.88, 0.9, 0.92, 0.93, 0.95]#[0.0, 0.3, 0.6, 0.9]\n",
    "    \n",
    "    \n",
    "    param_grid = dict(learn_rate=learn_rate, batch_size=batch_size, epochs=epochs, act_fn=act_fn, momentum=momentum)\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5,5), activation=params['act_fn'], input_shape=input_shape,name=\"1\"))\n",
    "    model.add(AveragePooling2D((2,2),name=\"2\"))\n",
    "    model.add(Conv2D(16, (5,5), activation=params['act_fn'],name=\"3\"))\n",
    "    model.add(AveragePooling2D((2,2),name=\"4\"))\n",
    "    model.add(Flatten(name=\"5\"))\n",
    "    model.add(Dense(120, activation=params['act_fn'],name=\"6\"))\n",
    "    model.add(Dense(84, activation=params['act_fn'],name=\"7\"))\n",
    "    model.add(Dense(num_classes, activation='softmax',name=\"f\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-danger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tfds.as_numpy(tfds.load(dataset_name,\n",
    "                                                               split=['train','test'],\n",
    "                                                               batch_size=-1,\n",
    "                                                               as_supervised=True))\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "#x_train = np.expand_dims(x_train, -1)\n",
    "#x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "y_test_uncat = y_test\n",
    "\n",
    "#to use with mean sq error\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "#shuffle both train and test once (to average between runs later on..)\n",
    "shuffler = np.random.permutation(len(x_train))\n",
    "x_train = x_train[shuffler]\n",
    "y_train = y_train[shuffler]\n",
    "shuffler = np.random.permutation(len(x_test))\n",
    "x_test = x_test[shuffler]\n",
    "y_test = y_test[shuffler]\n",
    "\n",
    "sample_height = x_train[0].shape[0]\n",
    "sample_width = x_train[0].shape[1]\n",
    "sample_channels = x_train[0].shape[2]\n",
    "\n",
    "\n",
    "input_shape = (sample_height, sample_width, sample_channels)\n",
    "\n",
    "def randomSplitClientsData(data,labels,numParties):\n",
    "    numSamplesPerClient = int(data.shape[0]/numParties)\n",
    "    print(numSamplesPerClient)\n",
    "    clientsData = np.zeros((numParties,int(numSamplesPerClient),sample_height,sample_width,sample_channels))\n",
    "    clientsDataLabels = np.zeros((numParties,int(numSamplesPerClient),num_classes))\n",
    "    #print(numSamplesPerClient)\n",
    "    ind = 0\n",
    "    for i in range(numParties):\n",
    "        clientsData[i] = data[ind:ind+numSamplesPerClient]\n",
    "        clientsDataLabels[i]=labels[ind:ind+numSamplesPerClient]\n",
    "        ind = ind+numSamplesPerClient\n",
    "    return clientsData, clientsDataLabels\n",
    "\n",
    "def prepare_data_for_X_clients(numClients):\n",
    "    clientsData, clientsDataLabels = randomSplitClientsData(x_train, y_train, numClients)\n",
    "    return clientsData, clientsDataLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsData, clientsDataLabels = prepare_data_for_X_clients(numClients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,7))\n",
    "for i in range(numClients):\n",
    "    client_labels = clientsDataLabels[i]\n",
    "    plot_data = collections.defaultdict(list)\n",
    "        \n",
    "    for l in client_labels:\n",
    "        plot_data[np.argmax(l)].append(np.argmax(l))\n",
    "    plt.subplot(int(math.ceil(numClients/3)), 3, i+1)\n",
    "    plt.title('Client {}'.format(i))\n",
    "    for j in range(numClients):\n",
    "        plt.hist(\n",
    "            plot_data[j],\n",
    "            density=False,\n",
    "            bins=list(range(num_classes+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[r])\n",
    "_ = plt.show()\n",
    "print(np.argmax(y_train[r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history, params):\n",
    "    print('##########################################################')\n",
    "    print(params)\n",
    "    # plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('MSE')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    #filename = sys.argv[0].split('/')[-1]\n",
    "    #pyplot.savefig(filename + '_plot.png')\n",
    "    #pyplot.close()\n",
    "    plt.show()\n",
    "    print('##########################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "hist_params = []\n",
    "\n",
    "def experiment(x_train, y_train, x_val, y_val, params):\n",
    "        \n",
    "    \n",
    "    optimizer = SGD(learning_rate=params['learn_rate'], momentum=params['momentum'], nesterov=False, name='SGD')\n",
    "    \n",
    "    model = get_model(params)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall(),tf.keras.metrics.Precision()],\n",
    "                  run_eagerly=False)\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\n",
    "                                                  min_delta=0.01,\n",
    "                                                  patience=5)\n",
    "    history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    callbacks=[early_stop],\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=0)\n",
    "\n",
    "    hist.append(history)\n",
    "    hist_params.append(params)\n",
    "        \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_for_X_clients(numClients, clientsData, clientsDataLabels, param_grid):\n",
    "\n",
    "    scan_res = np.zeros(numClients, dtype=object)\n",
    "    \n",
    "    \n",
    "    def client_gridsearch(i):\n",
    "        #Distribute load accross GPUs\n",
    "        with tf.device('/GPU:'+str(i%numGPUs)):\n",
    "        \n",
    "            print('training in client ', i)\n",
    "            scan_results = talos.Scan(x=clientsData[i],\n",
    "                                      y=clientsDataLabels[i],\n",
    "                                      params=param_grid,\n",
    "                                      model=experiment,\n",
    "                                      experiment_name=experiment_name)\n",
    "            scan_res[i]=scan_results\n",
    "        \n",
    "    # Batch multithreading\n",
    "    n_batch = int(math.ceil(float(numClients)/(numGPUs)))\n",
    "    print(n_batch)\n",
    "    remaining_clients = numClients\n",
    "    for i in range(n_batch):\n",
    "        threads = list()\n",
    "\n",
    "        for j in range(min(numGPUs, remaining_clients)):\n",
    "            t = threading.Thread(target=client_gridsearch, args=(i*(numGPUs)+j,))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "        for _,t in enumerate(threads):\n",
    "            remaining_clients -= 1\n",
    "            t.join()\n",
    "    \n",
    "    return scan_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-essay",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_res = []\n",
    "hist = []\n",
    "hist_params = []\n",
    "    \n",
    "param_grid = get_params_for_X_clients(numClients)\n",
    "\n",
    "res = grid_search_for_X_clients(numClients, clientsData, clientsDataLabels, param_grid)\n",
    "\n",
    "big_res=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-knight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sort dataframes\n",
    "sorted_data = []\n",
    "big_res = [r for r in big_res if not r == 0]\n",
    "\n",
    "for _,df in enumerate(big_res):\n",
    "    sorted_data.append(df.data.sort_values(by='val_accuracy',ascending=False).head())\n",
    "\n",
    "## Write dataframes to files\n",
    "for i,df in enumerate(sorted_data):\n",
    "    df.to_csv(experiment_name+\"_res/res\"+str(numClients)+\"/res\"+str(numClients)+\"_client_\"+str(i)+\".csv\")\n",
    "\n",
    "for i in range(len(sorted_data)):\n",
    "    with pandas.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        display(sorted_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-smoke",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_params = np.zeros(4, dtype=float) #[lr, batchsize, epochs, momentum]\n",
    "\n",
    "for _,client_data in enumerate(sorted_data):\n",
    "    avg_params[0] += client_data.head(1)['learn_rate'].item()\n",
    "    avg_params[1] += client_data.head(1)['batch_size'].item()\n",
    "    avg_params[2] += client_data.head(1)['round_epochs'].item()\n",
    "    avg_params[3] += client_data.head(1)['momentum'].item()\n",
    "    \n",
    "avg_params = avg_params / len(sorted_data)\n",
    "\n",
    "print(\"Avg lr:\", avg_params[0], \"Avg batchsize:\", int(math.ceil(avg_params[1])), \"Avg epochs:\", int(math.ceil(avg_params[2])), \"Avg momentum:\",avg_params[3])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_diagnostics(hist[0], hist_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics_for_X_clients(numClients, clientsData, clientsDataLabels, avg_test_params):\n",
    "\n",
    "    metrics_res = np.zeros(numClients, dtype=object)\n",
    "    \n",
    "    \n",
    "    def client_test_metrics(i):\n",
    "        #Distribute load accross GPUs\n",
    "        with tf.device('/GPU:'+str(i%numGPUs)):\n",
    "\n",
    "            print('training in client ', i)\n",
    "            optimizer = SGD(learning_rate=avg_test_params['learn_rate'], momentum=avg_test_params['momentum'], nesterov=False, name='SGD')\n",
    "\n",
    "\n",
    "            model = get_model(avg_test_params)\n",
    "            model.compile(optimizer=optimizer, loss=\"mean_squared_error\", metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "\n",
    "            model.fit(x=clientsData[i],\n",
    "                        y=clientsDataLabels[i],\n",
    "                        epochs=avg_test_params['epochs'],\n",
    "                        batch_size=avg_test_params['batch_size'],\n",
    "                        verbose=1)\n",
    "            print('evaluation in client ', i)\n",
    "            metrics = model.evaluate(x_test, y_test)\n",
    "            metrics_res[i] = metrics\n",
    "            \n",
    "        \n",
    "    # Batch multithreading\n",
    "    n_batch = int(math.ceil(float(numClients)/(numGPUs)))\n",
    "    print(n_batch)\n",
    "    remaining_clients = numClients\n",
    "    for i in range(n_batch):\n",
    "        threads = list()\n",
    "\n",
    "        for j in range(min(numGPUs, remaining_clients)):\n",
    "            t = threading.Thread(target=client_test_metrics, args=(i*(numGPUs)+j,))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "        for _,t in enumerate(threads):\n",
    "            remaining_clients -= 1\n",
    "            t.join()\n",
    "    \n",
    "    return metrics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-niagara",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### Retrain each client to run the test set and get best metrics\n",
    "avg_test_params = dict(learn_rate=avg_params[0], batch_size=int(math.ceil(avg_params[1])), epochs=int(math.ceil(avg_params[2])), act_fn='relu', momentum=avg_params[3])\n",
    "avg_test_res = []\n",
    "\n",
    "clientsData, clientsDataLabels = prepare_data_for_X_clients(numClients)\n",
    "\n",
    "res = test_metrics_for_X_clients(numClients, clientsData, clientsDataLabels, avg_test_params)\n",
    "\n",
    "avg_test_res=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "best_precision = 0.0\n",
    "best_recall = 0.0\n",
    "\n",
    "avg_test_res = [r for r in avg_test_res if not r == 0]\n",
    "\n",
    "for _,client_metrics in enumerate(avg_test_res):\n",
    "    if client_metrics[1] >= best_val_acc:\n",
    "        best_val_acc = client_metrics[1]\n",
    "        best_precision = client_metrics[2]\n",
    "        best_recall = client_metrics[3]\n",
    "    \n",
    "print(\"Best val_acc: \", best_val_acc)\n",
    "print(\"Best precision: \", best_precision)\n",
    "print(\"Best recall: \", best_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Activation functions optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_fn = [\"relu\", \"sigmoid\", \"tanh\"]\n",
    "act_fn_params = dict(learn_rate=[avg_params[0]], batch_size=[int(math.ceil(avg_params[1]))], epochs=[int(math.ceil(avg_params[2]))],momentum=[avg_params[3]], act_fn=act_fn)\n",
    "act_fn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-bearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "act_fn_big_res = []\n",
    "hist = []\n",
    "hist_params = []\n",
    "    \n",
    "clientsData, clientsDataLabels = prepare_data_for_X_clients(numClients)\n",
    "param_grid = get_params_for_X_clients(numClients)\n",
    "\n",
    "res = grid_search_for_X_clients(numClients, clientsData, clientsDataLabels, act_fn_params)\n",
    "\n",
    "act_big_res=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-principal",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Sort dataframes\n",
    "act_big_res = [r for r in act_big_res if not r == 0]\n",
    "sorted_data_activation = []\n",
    "for _,df in enumerate(act_big_res):\n",
    "    sorted_data_activation.append(df.data.sort_values(by='val_accuracy',ascending=False))\n",
    "\n",
    "for i in range(len(sorted_data_activation)):\n",
    "    with pandas.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        display(sorted_data_activation[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get best activation function\n",
    "act_fn_count = np.zeros(len(act_fn), dtype=float)\n",
    "\n",
    "for _,client_data in enumerate(sorted_data_activation):\n",
    "    for i,fn in enumerate(act_fn):\n",
    "        if client_data.head(1)['act_fn'].item() == fn:\n",
    "            act_fn_count[i] += 1\n",
    "            \n",
    "best_act_fn = act_fn[np.argmax(act_fn_count)]\n",
    "print(\"Best activation function :\", best_act_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-science",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
